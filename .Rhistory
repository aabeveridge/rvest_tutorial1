library(rvest)
library(tidyverse)
main.url <- read_html("https://greensboro.com/news/local_news/")
scrape.list <- html_nodes(main.url, xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "tnt-asset-type-article", " " )) and contains(concat( " ", @class, " " ), concat( " ", "image-left", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "tnt-asset-link", " " ))]') %>%
  html_attr("href")
head(scrape.list)
typeof(scrape.list)
class(scrape.list)
//*[@id="asset-05387318-ecb3-11ea-8fb1-fbfc416e7dc4"]/div[2]/header/div/span/ul/li[3]/time  new.url <- read_html(scrape.list[1])
  new.url <- read_html(scrape.list[1])
for (i in seq_along(scrape.list)) {
  scrape.list[i] <- paste0('https://greensboro.com', scrape.list[i])
}
  new.url <- read_html(scrape.list[1])
data.add <- html_nodes(new.url, xpath='//*[@id="asset-05387318-ecb3-11ea-8fb1-fbfc416e7dc4"]/div[2]/header/div/span/ul/li[3]/time')
data.add
data.add[1]
data.add <- html_nodes(new.url) %>%
  html_attr("datetime")
data.add <- html_nodes(new.url, "hidden-print") %>%
  html_attr("datetime")
data.add
data.add[1]
data.add <- html_nodes(new.url, "tnt-date asset-date text-muted") %>%
  html_attr("datetime")
data.add <- html_nodes(new.url, "tnt-date asset-date text-muted") %>%
data.add[1]
?html_nodes
data.add <- html_nodes(new.url, css="tnt-date asset-date text-muted") %>%
  html_attr("datetime")
data.add[1]
data.add
data.add <- html_nodes(new.url, "time") %>%
  html_attr("datetime")
data.add
for (i in seq_along(scrape.list)) {
  new.url <- read_html(scrape.list[i])
  text.add <- html_nodes(new.url, xpath='//*[(@id = "asset-content")]//p') %>%
    html_text()
  text.add <- paste(text.add, collapse=" ")
  date.add <- html_nodes(new.url, "time") %>%
    html_attr("datetime")
  page.text <- c(page.text, text.add)
  page.date <- c(page.date, date.add)
}
page.text <- vector()
page.date <- vector()
for (i in seq_along(scrape.list)) {
  new.url <- read_html(scrape.list[i])
  text.add <- html_nodes(new.url, xpath='//*[(@id = "asset-content")]//p') %>%
    html_text()
  text.add <- paste(text.add, collapse=" ")
  date.add <- html_nodes(new.url, "time") %>%
    html_attr("datetime")
  page.text <- c(page.text, text.add)
  page.date <- c(page.date, date.add)
}
scrape.data <- tibble('url'=scrape.list, 'date'=page.date, 'text'=page.text)
  new.url <- read_html(scrape.list[1])
date.add <- html_nodes(new.url, "time") %>%
    html_attr("datetime")
date.add
class(date.add)
typeof(date.add)
unnest(date.add)
paste(data.add, collapse="")
length(date.add)
data.add <- data.add[-c(2)]
length(date.add)
data.add
length(date.add)
for (i in seq_along(scrape.list)) {
  new.url <- read_html(scrape.list[1])
  text.add <- html_nodes(new.url, xpath='//*[(@id = "asset-content")]//p') %>%
    html_text()
  text.add <- paste(text.add, collapse=" ")
  date.add <- html_nodes(new.url, "time") %>%
    html_attr("datetime")
  data.add <- data.add[-c(2)]
  page.text <- c(page.text, text.add)
  page.date <- c(page.date, date.add)
}
main.url <- read_html("https://greensboro.com/news/local_news/")
scrape.list <- html_nodes(main.url, xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "tnt-asset-type-article", " " )) and contains(concat( " ", @class, " " ), concat( " ", "image-left", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "tnt-asset-link", " " ))]') %>%
  html_attr("href")
for (i in seq_along(scrape.list)) {
  scrape.list[i] <- paste0('https://greensboro.com', scrape.list[i])
}
page.text <- vector()
page.date <- vector()
for (i in seq_along(scrape.list)) {
  new.url <- read_html(scrape.list[i])
  text.add <- html_nodes(new.url, xpath='//*[(@id = "asset-content")]//p') %>%
    html_text()
  text.add <- paste(text.add, collapse=" ")
  date.add <- html_nodes(new.url, "time") %>%
    html_attr("datetime")
  data.add <- data.add[-c(2)]
  page.text <- c(page.text, text.add)
  page.date <- c(page.date, date.add)
}
length(data.add)
length(page.date)
# The for loop visits each URL in scrape.list and then collects the text content from each page, creating a new list
for (i in seq_along(scrape.list)) {
  new.url <- read_html(scrape.list[i])
  text.add <- html_nodes(new.url, xpath='//*[(@id = "asset-content")]//p') %>%
    html_text()
  text.add <- paste(text.add, collapse=" ")
  date.add <- html_nodes(new.url, "time") %>%
    html_attr("datetime")
  date.add <- date.add[-c(2)]
  page.text <- c(page.text, text.add)
  page.date <- c(page.date, date.add)
}
length(page.date)
page.text <- vector()
page.date <- vector()
for (i in seq_along(scrape.list)) {
  new.url <- read_html(scrape.list[i])
  text.add <- html_nodes(new.url, xpath='//*[(@id = "asset-content")]//p') %>%
    html_text()
  text.add <- paste(text.add, collapse=" ")
  date.add <- html_nodes(new.url, "time") %>%
    html_attr("datetime")
  date.add <- date.add[-c(2)]
  page.text <- c(page.text, text.add)
  page.date <- c(page.date, date.add)
}
length(date.add)
length(page.date))
length(page.date)
scrape.data <- tibble('url'=scrape.list, 'date'=page.date, 'text'=page.text)
scrape.data
tail(scrape.data)
write.csv(scrape.data, 'gboro_news.csv')
q()
